{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cosinuss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdYBJngOSFG6yWpCdDNI9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Floyer_Analysis/blob/main/Cosinuss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCjV4YaSzpT"
      },
      "source": [
        "#1. Set up dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_NhMO3JRJC0"
      },
      "source": [
        "import sys\r\n",
        "sys.path.append('C:/Users/57lzhang.US04WW4008/PycharmProjects/cosinuss')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buQcxbm5RvZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afa65ed-e5bc-4f8f-f0b6-e54d956d5cf4"
      },
      "source": [
        "from data_container import config, DataFile\r\n",
        "from data_container.api_db_sync import DBSync\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import datetime\r\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-03 14:21:02 - DEBUG -       dc_config:  45 - DcConfig initialized\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae-3xQw-S4E9"
      },
      "source": [
        "#2.Connect to Cosinuss Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlPl6rEfprbe"
      },
      "source": [
        "Open redis-server.exe first before running configure_api() "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU7_4mYhR1ou",
        "outputId": "6a5cf170-4d99-4212-fb94-8ef3d973488b"
      },
      "source": [
        "def configure_api(db_name, username, update_local=False):\r\n",
        "    \"\"\"\r\n",
        "    Configure api as the data handler\r\n",
        "    \"\"\"\r\n",
        "    try:\r\n",
        "        config.init(db_name=db_name)\r\n",
        "        api = DBSync(username=username,\r\n",
        "                    server = 'https://telecovid.earconnect.de')\r\n",
        "    except:\r\n",
        "        print('Configure API failed')\r\n",
        "        return\r\n",
        "    # password: teeshau7aiMonoh9ee\r\n",
        "    if update_local:\r\n",
        "    # download data from the server\r\n",
        "        api.pull_all_dfs(prj_hash_id='M9KH')\r\n",
        "    \r\n",
        "    df_list_local = api.df_list(prj_hash_id='M9KH')\r\n",
        "    print('There are', len(df_list_local), 'files in local database')\r\n",
        "    return api\r\n",
        "\r\n",
        "\r\n",
        "api = configure_api('sonova_analysis', 'sonova.fremont.api')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-03 14:21:06 -  INFO -       dc_config: 202 - data_path is \"C:\\Users\\57lzhang.US04WW4008\\PycharmProjects\\cosinuss\\data\"\n",
            "2021-03-03 14:21:06 -  INFO -       dc_config: 118 - connect to database \"sonova_analysis\"\n",
            "2021-03-03 14:21:06 -  INFO -       dc_config: 113 - init of data_container successful\n",
            "2021-03-03 14:21:06 -  INFO -       api_login: 241 - login: sonova.fremont.api @ https://telecovid.earconnect.de\n",
            "2021-03-03 14:21:07 -  INFO -       api_login: 253 - successful login, it expires 2021-03-04 02:21:08 expiring in 720.0 min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are 163 files in local database\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "4dKZ65Q_z5b-",
        "outputId": "097742a2-ab36-4c8a-dc43-661e9332fc2f"
      },
      "source": [
        "def generate_table(date):\r\n",
        "    \"\"\"\r\n",
        "    Convert local database into a Pandas dataframe, for ease of data analysis\r\n",
        "\r\n",
        "    params:\r\n",
        "    date: select the date that data is generated\r\n",
        "\r\n",
        "    outputs:\r\n",
        "    an overview table in the form of pandas dataframe\r\n",
        "    \"\"\"\r\n",
        "    table = api.overview_dfs()\r\n",
        "    overview_df = pd.DataFrame(table[1:], columns=table[0])\r\n",
        "    # change to lower case for the device name\r\n",
        "    overview_df['device'] = overview_df['device'].str.lower()\r\n",
        "    overview_df.loc[overview_df.device.str.match(r'^polar'), 'device']= 'polar'\r\n",
        "    overview_df = overview_df[overview_df.when > date]\r\n",
        "    overview_df['when'] = [datetime.datetime.strptime(i, '%Y-%m-%d %H:%M:%S') for \r\n",
        "                           i in list(overview_df['when'])]\r\n",
        "    overview_df['duration'] = [datetime.datetime.strptime(i, '%H:%M:%S') for \r\n",
        "                               i in list(overview_df['duration'])]\r\n",
        "    overview_df['end'] = [(overview_df.when.loc[i] \r\n",
        "                           - datetime.datetime(1900, 1, 1) \r\n",
        "                           + overview_df.duration.loc[i]) for i in overview_df.index]\r\n",
        "    return overview_df\r\n",
        "\r\n",
        "table = generate_table('2020-12-31')\r\n",
        "print('There are', len(table), 'test records')\r\n",
        "table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 63 test records\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               person                when  device              df id  \\\n",
              "100  M9KH.BZYG (RaHe) 2021-02-08 14:15:09   polar  1C2RATHBVH.C4CDEZ   \n",
              "101  M9KH.BZYG (RaHe) 2021-02-08 14:15:14  cshell  1C2RATHBVH.38CD8K   \n",
              "102  M9KH.BZYG (RaHe) 2021-02-08 14:15:23  cshell  1C2RATHBVH.LEMWKL   \n",
              "103  M9KH.BZYG (RaHe) 2021-02-08 14:15:35  garmin  8QEK89R9B4.NZH5XL   \n",
              "104  M9KH.ME9A (LeLo) 2021-02-08 15:23:00   polar  1C2RATHBVH.XCMFW1   \n",
              "\n",
              "               duration samples                                 cols  \\\n",
              "100 1900-01-01 00:34:48   2.1 K                           heart_rate   \n",
              "101 1900-01-01 00:34:42   1.7 M  ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...   \n",
              "102 1900-01-01 00:34:36   1.7 M  ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...   \n",
              "103 1900-01-01 00:45:27   2.7 K                           heart_rate   \n",
              "104 1900-01-01 00:34:34   2.1 K                           heart_rate   \n",
              "\n",
              "                    end  \n",
              "100 2021-02-08 14:49:57  \n",
              "101 2021-02-08 14:49:56  \n",
              "102 2021-02-08 14:49:59  \n",
              "103 2021-02-08 15:01:02  \n",
              "104 2021-02-08 15:57:34  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "      <th>when</th>\n",
              "      <th>device</th>\n",
              "      <th>df id</th>\n",
              "      <th>duration</th>\n",
              "      <th>samples</th>\n",
              "      <th>cols</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:09</td>\n",
              "      <td>polar</td>\n",
              "      <td>1C2RATHBVH.C4CDEZ</td>\n",
              "      <td>1900-01-01 00:34:48</td>\n",
              "      <td>2.1 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 14:49:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:14</td>\n",
              "      <td>cshell</td>\n",
              "      <td>1C2RATHBVH.38CD8K</td>\n",
              "      <td>1900-01-01 00:34:42</td>\n",
              "      <td>1.7 M</td>\n",
              "      <td>ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...</td>\n",
              "      <td>2021-02-08 14:49:56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:23</td>\n",
              "      <td>cshell</td>\n",
              "      <td>1C2RATHBVH.LEMWKL</td>\n",
              "      <td>1900-01-01 00:34:36</td>\n",
              "      <td>1.7 M</td>\n",
              "      <td>ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...</td>\n",
              "      <td>2021-02-08 14:49:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:35</td>\n",
              "      <td>garmin</td>\n",
              "      <td>8QEK89R9B4.NZH5XL</td>\n",
              "      <td>1900-01-01 00:45:27</td>\n",
              "      <td>2.7 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 15:01:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>M9KH.ME9A (LeLo)</td>\n",
              "      <td>2021-02-08 15:23:00</td>\n",
              "      <td>polar</td>\n",
              "      <td>1C2RATHBVH.XCMFW1</td>\n",
              "      <td>1900-01-01 00:34:34</td>\n",
              "      <td>2.1 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 15:57:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bovy12uwygfx"
      },
      "source": [
        " def correct_label(table):\r\n",
        "    \"\"\"\r\n",
        "    fix the labeling problem of cshell and biometric\r\n",
        "    \r\n",
        "    params:\r\n",
        "    overview table from generate_table function\r\n",
        "\r\n",
        "    outputs:\r\n",
        "    updated table with correct cshell and biometric labels\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    def get_device_model(df):\r\n",
        "        \"\"\"\r\n",
        "        get device\r\n",
        "        \r\n",
        "        params:\r\n",
        "        df - output of api.pull_df\r\n",
        "        \r\n",
        "        outputs:\r\n",
        "        correct device label for df\r\n",
        "        \"\"\"\r\n",
        "        # get device\r\n",
        "        if df.device:\r\n",
        "            device_model = df.device.device_model\r\n",
        "        else:\r\n",
        "            device_model = df.device_model\r\n",
        "        \r\n",
        "        if 'polar' in device_model.lower():\r\n",
        "            return 'polar'\r\n",
        "\r\n",
        "        elif 'biomeric' in device_model.lower() or 'cshell' in device_model.lower():\r\n",
        "            if 'ppg_ir_2' in list(df.cols):\r\n",
        "                some_data = df.c.ppg_ir_2.y[600:650]\r\n",
        "                if list(some_data):\r\n",
        "                    ppg_mean = np.mean(some_data)\r\n",
        "                else:\r\n",
        "                    return 'na'\r\n",
        "            else:\r\n",
        "                ppg_mean = 1000\r\n",
        "            \r\n",
        "            if ppg_mean < 500:\r\n",
        "                return 'cshell'\r\n",
        "            else:\r\n",
        "                return 'biometric'\r\n",
        "    \r\n",
        "    def get_correct_label(hash_ids, target_device):\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        # output list of corrected labels\r\n",
        "        corrected_name = []\r\n",
        "        # a counter to record how many labels have been corrected after processing\r\n",
        "        cnt = 0\r\n",
        "        print(f'There are in total {len(hash_ids)} files with {target_device} label')\r\n",
        "        for i in range(len(hash_ids)):\r\n",
        "            try:\r\n",
        "                print(f'api reading {i}th file...')\r\n",
        "                df = api.pull_df(list(hash_ids)[i]) \r\n",
        "            # in case reading file failed\r\n",
        "            except:\r\n",
        "                print(f'api read {i}th file failed!')\r\n",
        "                corrected_name.append(table.loc[hash_ids.index[i]].device)\r\n",
        "                continue\r\n",
        "            # get the new label by api reading the file\r\n",
        "            new_label = get_device_model(df)\r\n",
        "            corrected_name.append(new_label)\r\n",
        "            if new_label != table.loc[hash_ids.index[i]].device:\r\n",
        "                print(f'Person {table.loc[hash_ids.index[i]].person} and \\\r\n",
        "            {target_device} label has been corrected to {new_label}')\r\n",
        "                cnt += 1\r\n",
        "        print(f'There are in total {cnt} files been corrected')\r\n",
        "        return corrected_name\r\n",
        "\r\n",
        "    # get the hash ids of cshell and biometric in original table\r\n",
        "    cshell_hash_ids = table[table.device == 'cshell']['df id']\r\n",
        "    biometric_hash_ids = table[table.device == 'biometric']['df id']\r\n",
        "\r\n",
        "    # get the row index of cshell and biometric in original table\r\n",
        "    original_cshell_index = table[table.device == 'cshell'].index\r\n",
        "    original_biometric_index = table[table.device == 'biometric'].index\r\n",
        "    \r\n",
        "    # update the original table with corrected labels of cshell and biometric\r\n",
        "    table.device.loc[original_cshell_index] = get_correct_label(cshell_hash_ids, \r\n",
        "                                                     'cshell')\r\n",
        "    table.device.loc[original_biometric_index] = get_correct_label(biometric_hash_ids, \r\n",
        "                                                        'biometric')\r\n",
        "\r\n",
        "    return table\r\n",
        "\r\n",
        "table = correct_label(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XkqoXp1rFQ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a83f4240-72ee-42a7-9d51-87208be2827e"
      },
      "source": [
        "table.to_pickle(\"C:/Users/57lzhang.US04WW4008/PycharmProjects/cosinuss/local_db.pkl\")\r\n",
        "#load table from local directory\r\n",
        "table = pd.read_pickle(\"C:/Users/57lzhang.US04WW4008/PycharmProjects/cosinuss/local_db.pkl\")\r\n",
        "table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               person                when     device              df id  \\\n",
              "100  M9KH.BZYG (RaHe) 2021-02-08 14:15:09      polar  1C2RATHBVH.C4CDEZ   \n",
              "101  M9KH.BZYG (RaHe) 2021-02-08 14:15:14     cshell  1C2RATHBVH.38CD8K   \n",
              "102  M9KH.BZYG (RaHe) 2021-02-08 14:15:23  biometric  1C2RATHBVH.LEMWKL   \n",
              "103  M9KH.BZYG (RaHe) 2021-02-08 14:15:35     garmin  8QEK89R9B4.NZH5XL   \n",
              "104  M9KH.ME9A (LeLo) 2021-02-08 15:23:00      polar  1C2RATHBVH.XCMFW1   \n",
              "\n",
              "               duration samples                                 cols  \\\n",
              "100 1900-01-01 00:34:48   2.1 K                           heart_rate   \n",
              "101 1900-01-01 00:34:42   1.7 M  ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...   \n",
              "102 1900-01-01 00:34:36   1.7 M  ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...   \n",
              "103 1900-01-01 00:45:27   2.7 K                           heart_rate   \n",
              "104 1900-01-01 00:34:34   2.1 K                           heart_rate   \n",
              "\n",
              "                    end  \n",
              "100 2021-02-08 14:49:57  \n",
              "101 2021-02-08 14:49:56  \n",
              "102 2021-02-08 14:49:59  \n",
              "103 2021-02-08 15:01:02  \n",
              "104 2021-02-08 15:57:34  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "      <th>when</th>\n",
              "      <th>device</th>\n",
              "      <th>df id</th>\n",
              "      <th>duration</th>\n",
              "      <th>samples</th>\n",
              "      <th>cols</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:09</td>\n",
              "      <td>polar</td>\n",
              "      <td>1C2RATHBVH.C4CDEZ</td>\n",
              "      <td>1900-01-01 00:34:48</td>\n",
              "      <td>2.1 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 14:49:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:14</td>\n",
              "      <td>cshell</td>\n",
              "      <td>1C2RATHBVH.38CD8K</td>\n",
              "      <td>1900-01-01 00:34:42</td>\n",
              "      <td>1.7 M</td>\n",
              "      <td>ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...</td>\n",
              "      <td>2021-02-08 14:49:56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:23</td>\n",
              "      <td>biometric</td>\n",
              "      <td>1C2RATHBVH.LEMWKL</td>\n",
              "      <td>1900-01-01 00:34:36</td>\n",
              "      <td>1.7 M</td>\n",
              "      <td>ppg_ir, ppg_ir_2, ppg_ir_3, ppg_...</td>\n",
              "      <td>2021-02-08 14:49:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>M9KH.BZYG (RaHe)</td>\n",
              "      <td>2021-02-08 14:15:35</td>\n",
              "      <td>garmin</td>\n",
              "      <td>8QEK89R9B4.NZH5XL</td>\n",
              "      <td>1900-01-01 00:45:27</td>\n",
              "      <td>2.7 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 15:01:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>M9KH.ME9A (LeLo)</td>\n",
              "      <td>2021-02-08 15:23:00</td>\n",
              "      <td>polar</td>\n",
              "      <td>1C2RATHBVH.XCMFW1</td>\n",
              "      <td>1900-01-01 00:34:34</td>\n",
              "      <td>2.1 K</td>\n",
              "      <td>heart_rate</td>\n",
              "      <td>2021-02-08 15:57:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nSKkqBi1THp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d772ad62-14f3-466c-9191-e73b81fbd189"
      },
      "source": [
        "def find_pairs_row_index(table, floyer_device='cshell'):\r\n",
        "    \"\"\"\r\n",
        "    Filter table with paired polar and floyer devices\r\n",
        "    \"\"\"\r\n",
        "    paired_table = table[table.device.isin(['polar', floyer_device])]\r\n",
        "    \r\n",
        "    paired_idx_lst = []\r\n",
        "    # find paried sample ids\r\n",
        "    for person in list(paired_table['person'].unique()):\r\n",
        "        person_paired_table = paired_table[paired_table.person == person]\r\n",
        "        polar_idx_lst = person_paired_table[person_paired_table.device == 'polar'].index\r\n",
        "        floyer_idx_lst = person_paired_table[person_paired_table.device == floyer_device].index\r\n",
        "\r\n",
        "        # find pairs by checking the overlapping time (2*O(n^2))\r\n",
        "        for polar_idx in polar_idx_lst:\r\n",
        "            polar_start_time = person_paired_table.when.loc[polar_idx]\r\n",
        "            polar_end_time = person_paired_table.end.loc[polar_idx]\r\n",
        "\r\n",
        "            for floyer_idx in floyer_idx_lst:\r\n",
        "                floyer_start_time = person_paired_table.when.loc[floyer_idx]\r\n",
        "                floyer_end_time = person_paired_table.end.loc[floyer_idx]\r\n",
        "                \r\n",
        "                if floyer_start_time >= polar_end_time or polar_start_time >= floyer_end_time:\r\n",
        "                    continue\r\n",
        "                else:\r\n",
        "                    paired_idx_lst.append([polar_idx, floyer_idx])\r\n",
        "    \r\n",
        "    return paired_idx_lst\r\n",
        "\r\n",
        "paired_idx_lst = find_pairs_row_index(table, floyer_device='cshell')\r\n",
        "paired_idx_lst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[100, 101],\n",
              " [104, 106],\n",
              " [117, 116],\n",
              " [120, 121],\n",
              " [126, 124],\n",
              " [130, 128],\n",
              " [142, 141],\n",
              " [144, 146],\n",
              " [148, 150],\n",
              " [160, 162]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAfUe0gt6-Yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac2db26-5a52-4757-ab69-af2aec94f4e2"
      },
      "source": [
        "def generate_paired_samples(paired_idx_lst, table, hr_algo_version):\r\n",
        "    \"\"\"\r\n",
        "    Use the index of paired devices (output from find_pairs_row_index) to generate\r\n",
        "    a list of PairedSample instances\r\n",
        "\r\n",
        "    params:\r\n",
        "    paired_idx_lst: a list containing the row index of paired devices, \r\n",
        "                    It is the output of function find_pairs_row_index.\r\n",
        "    table: a Pandas dataframe that stores test infomation, it is the output of the\r\n",
        "           correct_label() \r\n",
        "\r\n",
        "    outputs:\r\n",
        "    A list of PairedSample instances\r\n",
        "    \"\"\"\r\n",
        "    paired_sample_lst = []\r\n",
        "    for polar_idx, floyer_idx in paired_idx_lst:\r\n",
        "        polar_hash_id = table['df id'].loc[polar_idx]\r\n",
        "        floyer_hash_id = table['df id'].loc[floyer_idx]\r\n",
        "        paired_sample_lst.append(PairedSample(polar_hash_id, floyer_hash_id, hr_algo_version))\r\n",
        "    return paired_sample_lst\r\n",
        "\r\n",
        "paired_sample_lst = generate_paired_samples(paired_idx_lst, table, 'v0')\r\n",
        "# save the paried sample list to save time on next run\r\n",
        "with open(\"C:/Users/57lzhang.US04WW4008/PycharmProjects/cosinuss/paired_sample.pkl\", \"wb\") as fp:\r\n",
        "    pickle.dump(paired_sample_lst, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-01 15:24:25 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.C4CDEZ completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:25:11 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.38CD8K completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:25:14 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.XCMFW1 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:25:57 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.G2HHR3 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:26:01 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.2UJU2G completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:26:53 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.009DDE completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:26:55 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.58DZ5Z completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:28:00 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.V5BXYZ completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:28:03 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.60SWCR completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:29:16 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.C0KF6Y completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:29:19 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.4NGWZ3 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:30:07 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.DYPFM1 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:30:10 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.QNW02T completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:30:49 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.JYKU22 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:30:52 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.08ZUGQ completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:31:22 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.N74KKE completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:31:26 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.GND7BQ completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:31:56 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.2E4QXX completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:31:59 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.ZBY97Y completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n",
            "2021-03-01 15:32:36 -  INFO -     api_db_sync: 429 - Data file 1C2RATHBVH.8HL047 completely downloaded + database_entries: Scope, Config, Projects (4), Receivers (3), Persons (27), Devices (4)User (6), \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFT24cesFnOr"
      },
      "source": [
        "with open(\"C:/Users/57lzhang.US04WW4008/PycharmProjects/cosinuss/paired_sample.pkl\", \"rb\") as fp:\r\n",
        "    paired_sample_lst = pickle.load(fp)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cuc93RnZfeZ"
      },
      "source": [
        "##Continue here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4tiEPeVwGrP",
        "outputId": "aa989615-6343-4fd8-846d-50ae7c76349b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "paired_sample.header_floyer.chunks_labelled"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=0, date_time_start=2021-02-08 15:23:50.381000+01:00, time_offset=34.91, duration=182.102)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=1, date_time_start=2021-02-08 15:26:52.483000+01:00, time_offset=217.012, duration=186.274)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=2, date_time_start=2021-02-08 15:29:58.757000+01:00, time_offset=403.286, duration=181.036)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=3, date_time_start=2021-02-08 15:32:59.793000+01:00, time_offset=584.322, duration=181.544)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=4, date_time_start=2021-02-08 15:37:06.381000+01:00, time_offset=830.91, duration=290.638)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=5, date_time_start=2021-02-08 15:41:57.019000+01:00, time_offset=1121.548, duration=318.712)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=6, date_time_start=2021-02-08 15:47:15.731000+01:00, time_offset=1440.26, duration=289.039)>,\n",
              " <DataChunk: LabelledChunk(df_hash=1C2RATHBVH.G2HHR3, index=7, date_time_start=2021-02-08 15:52:04.770000+01:00, time_offset=1729.299, duration=300.0)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lior0l3-oBwW"
      },
      "source": [
        "# read polar data\r\n",
        "polar = DataFile.objects(_hash_id=paired_sample.polar_hash_id).first()\r\n",
        "polar_hr_x = polar.c.heart_rate.x\r\n",
        "polar_hr_y = polar.c.heart_rate.y\r\n",
        "\r\n",
        "# process floyer data with heart rate algorithm\r\n",
        "floyer = api.one3_hr_algo(paired_sample.hr_algo_version, paired_sample.floyer_hash_id)\r\n",
        "floyer_hr_x = np.asarray(floyer['heart_rate_t'])\r\n",
        "floyer_hr_y = floyer['heart_rate']\r\n",
        "floyer_quality = floyer['quality']\r\n",
        "\r\n",
        "offset = (paired_sample.header_floyer.date_time_start - paired_sample.header_polar.date_time_start).total_seconds()\r\n",
        "\r\n",
        "# get the activity label\r\n",
        "activity_label = ['Unknown'] * len(floyer_hr_x)\r\n",
        "for i in range(len(paired_sample.header_floyer.chunks_labelled)):\r\n",
        "    i_th_activity_start = paired_sample.header_floyer.chunks_labelled[i].time_offset\r\n",
        "    i_th_activity_duration = paired_sample.header_floyer.chunks_labelled[i].duration\r\n",
        "    i_th_activity_end = i_th_activity_start + i_th_activity_duration\r\n",
        "    \r\n",
        "    activity_label[round(i_th_activity_start):round(i_th_activity_end)] = \\\r\n",
        "    [paired_sample.header_floyer.chunk_labels[i]] * (round(i_th_activity_end) - round(i_th_activity_start))\r\n",
        " \r\n",
        "\r\n",
        "# if floyer starts later\r\n",
        "if offset > 0:\r\n",
        "    polar_hr = polar_hr_y[int(offset):]\r\n",
        "# if polar starts later\r\n",
        "else:\r\n",
        "    floyer_hr = floyer_hr_y[int(offset):]\r\n",
        "    floyer_qa = floyer_quality[int(offset):]\r\n",
        "    floyer_activity = activity_label[int(offset):]\r\n",
        "\r\n",
        "    duration = min(len(polar_hr), len(floyer_hr_y))\r\n",
        "    final_polar_hr = polar_hr_y[:duration]\r\n",
        "    final_floyer_hr = floyer_hr[:duration]\r\n",
        "    final_floyer_quality = floyer_qa[:duration]\r\n",
        "    final_floyer_activity = floyer_activity[:duration]\r\n",
        "\r\n",
        "dic = {'floyer_hr': final_floyer_hr,\r\n",
        "       'polar_hr': final_polar_hr,\r\n",
        "       'quality': final_floyer_quality,\r\n",
        "       'activity': final_floyer_activity\r\n",
        "       }\r\n",
        "\r\n",
        "df = pd.DataFrame(data=dic)"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snhdvNNoYNOI",
        "outputId": "0b49e9be-120e-4898-e4e2-fa9775629d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1ZdxwuVgZm"
      },
      "source": [
        "##process the df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf90w8z9mrQ_"
      },
      "source": [
        "class PairedSample:\r\n",
        "    \r\n",
        "    def __init__(self, polar_hash_id, floyer_hash_id, hr_algo_version):\r\n",
        "        self.polar_hash_id = polar_hash_id\r\n",
        "        self.floyer_hash_id = floyer_hash_id\r\n",
        "        self.header_polar = api.pull_df(polar_hash_id)\r\n",
        "        self.header_floyer = api.pull_df(floyer_hash_id)\r\n",
        "        self.person_id = self.header_floyer.person.hash_id\r\n",
        "        self.hr_algo_version = hr_algo_version\r\n",
        "        self.df = pd.DataFrame()\r\n",
        "    \r\n",
        "    def process(self):\r\n",
        "        \"\"\"\r\n",
        "        Generate a pandas table for data analysis\r\n",
        "        \"\"\"\r\n",
        "        # read polar data\r\n",
        "        polar = DataFile.objects(_hash_id=self.polar_hash_id).first()\r\n",
        "        polar_hr_x = polar.c.heart_rate.x\r\n",
        "        polar_hr_y = polar.c.heart_rate.y\r\n",
        "\r\n",
        "        # process floyer data with heart rate algorithm\r\n",
        "        floyer = api.one3_hr_algo(self.hr_algo_version, self.floyer_hash_id)\r\n",
        "        floyer_hr_x = np.asarray(floyer['heart_rate_t'])\r\n",
        "        floyer_hr_y = floyer['heart_rate']\r\n",
        "        floyer_quality = floyer['quality']\r\n",
        "\r\n",
        "        offset = (self.header_floyer.date_time_start - self.header_polar.date_time_start).total_seconds()\r\n",
        "\r\n",
        "        # get the activity label\r\n",
        "        activity_label = ['Unknown'] * len(floyer_hr_x)\r\n",
        "        for i in range(len(self.header_floyer.chunks_labelled)):\r\n",
        "            i_th_activity_start = self.header_floyer.chunks_labelled[i].time_offset\r\n",
        "            i_th_activity_duration = self.header_floyer.chunks_labelled[i].duration\r\n",
        "            i_th_activity_end = i_th_activity_start + i_th_activity_duration\r\n",
        "            \r\n",
        "            activity_label[round(i_th_activity_start):round(i_th_activity_end)] = \\\r\n",
        "            [self.header_floyer.chunk_labels[i]] * (round(i_th_activity_end) - round(i_th_activity_start))\r\n",
        "        \r\n",
        "        # if floyer starts later\r\n",
        "        if offset > 0:\r\n",
        "            polar_hr = polar_hr_y[int(offset):]\r\n",
        "        # if polar starts later\r\n",
        "        else:\r\n",
        "            floyer_hr = floyer_hr_y[int(offset):]\r\n",
        "            floyer_qa = floyer_quality[int(offset):]\r\n",
        "            floyer_activity = activity_label[int(offset):]\r\n",
        "\r\n",
        "        duration = min(len(polar_hr), len(floyer_hr_y))\r\n",
        "        final_polar_hr = polar_hr_y[:duration]\r\n",
        "        final_floyer_hr = floyer_hr[:duration]\r\n",
        "        final_floyer_quality = floyer_qa[:duration]\r\n",
        "        final_floyer_activity = floyer_activity[:duration]\r\n",
        "\r\n",
        "        # add person id\r\n",
        "        final_person_id = [len(final_floyer_hr)] * self.person_id\r\n",
        "\r\n",
        "        dic = {'floyer_hr': final_floyer_hr,\r\n",
        "               'polar_hr': final_polar_hr,\r\n",
        "               'quality': final_floyer_quality,\r\n",
        "               'activity': final_floyer_activity,\r\n",
        "               'person': final_person_id \r\n",
        "               }\r\n",
        "            \r\n",
        "        self.df = pd.DataFrame(data=dic)\r\n",
        "        \r\n",
        "        return self.df\r\n",
        "\r\n",
        "\r\n",
        "    def mape(self):\r\n",
        "        for i in range(len(self.df)):\r\n",
        "            sum = abs(self.df['floyer_hr'][i] - self.df['polar_hr'][i] / self.df['polar_hr']) * 100\r\n",
        "        mape = round(sum / len(self.df), 1)\r\n",
        "        return mape\r\n",
        "        \r\n",
        "    def icc(self):\r\n",
        "\r\n",
        "        return icc\r\n",
        "    \r\n",
        "    def da(self):\r\n",
        "        return da\r\n",
        "\r\n",
        "    def line_plot(self):\r\n",
        "        return \r\n",
        "        \r\n",
        "\r\n",
        "    def ba_plot(self):\r\n",
        "        return"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rHGGwC3MOUc"
      },
      "source": [
        "# discard data with bad quality\r\n",
        "def quality_filter(x_l, y1_l, y2_l, q_l, q_lim=0):\r\n",
        "\r\n",
        "    x_in = []\r\n",
        "    x_out = []\r\n",
        "    y1_r = []\r\n",
        "    y2_r = []\r\n",
        "    data_in = 0\r\n",
        "    data_out = 0\r\n",
        "\r\n",
        "    for i in range(len(y1_l)):\r\n",
        "\r\n",
        "        x = x_l[i]\r\n",
        "        y1 = y1_l[i]\r\n",
        "        y2 = y2_l[i]\r\n",
        "        q = q_l[i]\r\n",
        "\r\n",
        "        if q >= q_lim and y1 > 35:\r\n",
        "            data_in += 1\r\n",
        "            x_in.append(x)\r\n",
        "            y1_r.append(y1)\r\n",
        "            y2_r.append(y2)\r\n",
        "        else:\r\n",
        "            x_out.append(x)\r\n",
        "            data_out += 1\r\n",
        "\r\n",
        "    return x_in, x_out, y1_r, y2_r, data_in, data_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vh2qYuOMO-s"
      },
      "source": [
        "# for our script this snippet is embedded in some higher levels of for loops...\r\n",
        "# assume you collected the data (polar and biotronic) in a dict d\r\n",
        "for chk in d['data_algo'][algo_name]['labels']:\r\n",
        "    # extract win data from algo HR\r\n",
        "    algo_x, algo_y = extract_win(d['data_algo'][algo_name]['x'], d['data_algo'][algo_name]['y'], 6.0/60.0,\r\n",
        "                                 x_start=chk['s'], x_end=chk['e'])\r\n",
        "    # extract win data (quality) from algo HR\r\n",
        "    q_x, q_y = extract_win(d['data_algo'][algo_name]['x'], d['data_algo'][algo_name]['q'], 6.0/60.0,\r\n",
        "                                 x_start=chk['s'], x_end=chk['e'], mean='max')\r\n",
        "    # extract win data from polar HR\r\n",
        "    polar_x, polar_y = extract_win(d['data_polar']['x'], d['data_polar']['y'], 6.0/60.0,\r\n",
        "                                 x_start=chk['s'], x_end=chk['e'])\r\n",
        "    # quality filter the data before\r\n",
        "    x_in, x_out, algo_y, polar_y, data_in, data_out = quality_filter(algo_x[2:-2], algo_y[2:-2],\r\n",
        "                                                        polar_y[2:-2], q_y[2:-2],\r\n",
        "                                                                     q_lim=algo_dic['qlim'])\r\n",
        "    # count discarded win and the ones taken into account\r\n",
        "    d['data_algo'][algo_name]['x_in'] += x_in\r\n",
        "    d['data_algo'][algo_name]['x_out'] += x_out\r\n",
        "    # calc lost rate\r\n",
        "    try:\r\n",
        "        chk['data_lost'] = 100.0 * float(data_out) / (data_in + data_out)\r\n",
        "    except ZeroDivisionError:\r\n",
        "        chk['data_lost'] = 0\r\n",
        "    # calc mape\r\n",
        "    chk['mape'] = calc_mape(algo_y, polar_y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}